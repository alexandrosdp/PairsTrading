{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "from glob import glob\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('tardis_data/combined/NEXOUSDT_2024_Q1.csv.gz', compression='gzip')\n",
    "\n",
    "# # Drop unused columns\n",
    "# df = df.drop(columns=['exchange', 'local_timestamp', 'id'])\n",
    "\n",
    "# # Convert timestamp from microseconds\n",
    "# df['timestamp'] = pd.to_datetime(df['timestamp'], unit='us')\n",
    "\n",
    "# # Reorder columns\n",
    "# df = df[['timestamp', 'symbol', 'side', 'price', 'amount']]\n",
    "\n",
    "# # Set timestamp as index\n",
    "# df = df.set_index('timestamp').sort_index()\n",
    "\n",
    "# # Resample to 1-minute intervals\n",
    "# df_1min = df.resample('5min').agg({\n",
    "#     'price': 'last',\n",
    "#     'amount': 'sum',\n",
    "#     'side': 'last',\n",
    "#     'symbol': 'last'\n",
    "# }).dropna().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(df_1min['price'].head(100), color='blue', label='Price')\n",
    "# plt.xlabel('Time')\n",
    "# plt.ylabel('Price')\n",
    "# plt.title('Price over Time')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_liquid = pd.read_csv('tardis_data/combined/NEXOUSDT_2024_Q1.csv.gz', compression='gzip')\n",
    "\n",
    "# # Drop unused columns\n",
    "# df_liquid = df_liquid.drop(columns=['exchange', 'local_timestamp', 'id'])\n",
    "\n",
    "# # Convert timestamp from microseconds\n",
    "# df_liquid['timestamp'] = pd.to_datetime(df_liquid['timestamp'], unit='us')\n",
    "\n",
    "# # Reorder columns\n",
    "# df_liquid = df_liquid[['timestamp', 'symbol', 'side', 'price', 'amount']]\n",
    "\n",
    "# # Set timestamp as index\n",
    "# df_liquid = df_liquid.set_index('timestamp').sort_index()\n",
    "\n",
    "# # Resample to 1-minute intervals\n",
    "# df_liquid_1min = df_liquid.resample('5min').agg({\n",
    "#     'price': 'last',\n",
    "#     'amount': 'sum',\n",
    "#     'side': 'last',\n",
    "#     'symbol': 'last'\n",
    "# }).dropna().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(df_liquid_1min['price'].head(100), color='blue', label='Price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Count the number of times the price does not change over a 1-minute, 2-minute, and 5-minute interval\n",
    "\n",
    "# # Count 1-minute price holds\n",
    "# df_1m = df.set_index('timestamp').resample('1min').last().dropna().reset_index()\n",
    "# no_change_1m = (df['price'].diff() == 0).sum()\n",
    "\n",
    "# # Count 3-minute price holds\n",
    "# df_3m = df.set_index('timestamp').resample('3min').last().dropna().reset_index()\n",
    "# no_change_3m = (df_3m['price'].diff() == 0).sum()\n",
    "\n",
    "# # Count 5-minute price holds\n",
    "# df_5m = df.set_index('timestamp').resample('5min').last().dropna().reset_index()\n",
    "# no_change_5m = (df_5m['price'].diff() == 0).sum()\n",
    "\n",
    "# #Count 30-minute price holds\n",
    "# df_30m = df.set_index('timestamp').resample('30min').last().dropna().reset_index()\n",
    "# no_change_30m = (df_30m['price'].diff() == 0).sum()\n",
    "\n",
    "# print(f\"Price unchanged:\")\n",
    "# print(f\"  Over 1-minute intervals: {no_change_1m}\")\n",
    "# print(f\"  Over 3-minute intervals: {no_change_3m}\")\n",
    "# print(f\"  Over 5-minute intervals: {no_change_5m}\")\n",
    "# print(f\"  Over 30-minute intervals: {no_change_30m}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def combine_tardis_trades(file_paths, output_path=None):\n",
    "#     \"\"\"\n",
    "#     Combines multiple Tardis trades files (compressed .csv.gz) into a single DataFrame\n",
    "#     with 1-minute resampled last prices for each symbol.\n",
    "\n",
    "#     Parameters:\n",
    "#     - file_paths (list): List of full paths to the .csv.gz files.\n",
    "#     - output_path (str, optional): Path to save the final combined CSV file.\n",
    "\n",
    "#     Returns:\n",
    "#     - pd.DataFrame: Combined DataFrame with timestamp and resampled price columns per symbol.\n",
    "#     \"\"\"\n",
    "\n",
    "#     resampled_dfs = []\n",
    "\n",
    "#     print(\"ðŸ“Š Processing trade files:\")\n",
    "#     for file_path in tqdm(file_paths, desc=\"Files processed\"):\n",
    "#         # Read file\n",
    "#         df = pd.read_csv(file_path, compression='gzip')\n",
    "\n",
    "#         # Drop unused columns\n",
    "#         df = df.drop(columns=['exchange', 'local_timestamp', 'id'])\n",
    "\n",
    "#         # Convert timestamp from microseconds\n",
    "#         df['timestamp'] = pd.to_datetime(df['timestamp'], unit='us')\n",
    "\n",
    "#         # Reorder columns\n",
    "#         df = df[['timestamp', 'symbol', 'side', 'price', 'amount']]\n",
    "\n",
    "#         # Set timestamp as index\n",
    "#         df = df.set_index('timestamp').sort_index()\n",
    "\n",
    "#         # Resample to 1-minute intervals\n",
    "#         df_1min = df.resample('5min').agg({\n",
    "#             'price': 'last',\n",
    "#             'amount': 'sum',\n",
    "#             'side': 'last',\n",
    "#             'symbol': 'last'\n",
    "#         }).dropna().reset_index()\n",
    "\n",
    "#         # Rename price column to last_price\n",
    "#         df_1min = df_1min.rename(columns={'price': 'last_price'})\n",
    "\n",
    "#         # Rename for final merge: SYMBOL/USDT_2024_1m\n",
    "#         symbol = df_1min['symbol'].iloc[0]\n",
    "#         df_1min = df_1min.rename(columns={'last_price': f'{symbol}_2024_5m'})\n",
    "\n",
    "#         # Reduce to timestamp + price column\n",
    "#         df_1min = df_1min[['timestamp', f'{symbol}_2024_5m']]\n",
    "\n",
    "#         # Store for merging later\n",
    "#         resampled_dfs.append(df_1min)\n",
    "\n",
    "#     print(\"ðŸ”— Merging DataFrames...\")\n",
    "#     df_combined = reduce(lambda left, right: pd.merge(left, right, on='timestamp', how='inner'), resampled_dfs)\n",
    "\n",
    "#     if output_path:\n",
    "#         df_combined.to_csv(output_path, index=False, compression='gzip')\n",
    "#         print(f\"âœ… Final combined file saved to {output_path}\")\n",
    "\n",
    "#     return df_combined\n",
    "\n",
    "def combine_tardis_trades(file_paths, output_path=None):\n",
    "    \"\"\"\n",
    "    Combines large Tardis trades files into a single DataFrame with 5-minute\n",
    "    resampled last prices per symbol. Optimized for low memory usage.\n",
    "    \"\"\"\n",
    "\n",
    "    combined_df = None  # Start with nothing\n",
    "\n",
    "    print(\"ðŸ“Š Processing and merging trade files one-by-one:\")\n",
    "    for file_path in tqdm(file_paths, desc=\"Files processed\"):\n",
    "        # Read and process each file\n",
    "        df = pd.read_csv(file_path, compression='gzip')\n",
    "\n",
    "        df = df.drop(columns=['exchange', 'local_timestamp', 'id'])\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='us')\n",
    "        df = df[['timestamp', 'symbol', 'side', 'price', 'amount']]\n",
    "        df = df.set_index('timestamp').sort_index()\n",
    "\n",
    "        df_1min = df.resample('5min').agg({\n",
    "            'price': 'last',\n",
    "            'amount': 'sum',\n",
    "            'side': 'last',\n",
    "            'symbol': 'last'\n",
    "        }).dropna().reset_index()\n",
    "\n",
    "        symbol = df_1min['symbol'].iloc[0]\n",
    "        price_col = f'{symbol}_2024_5m'\n",
    "        df_1min = df_1min.rename(columns={'price': price_col})\n",
    "        df_1min = df_1min[['timestamp', price_col]]\n",
    "        \n",
    "        #Merge is now done incrementally\n",
    "        # Merge into combined_df\n",
    "        if combined_df is None:\n",
    "            combined_df = df_1min\n",
    "        else:\n",
    "            combined_df = pd.merge(combined_df, df_1min, on='timestamp', how='inner')\n",
    "\n",
    "    if output_path:\n",
    "        combined_df.to_csv(output_path, index=False, compression='gzip')\n",
    "        print(f\"âœ… Final combined file saved to {output_path}\")\n",
    "\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function to calculate the daily average trading volume for each ticker\n",
    "\n",
    "def calculate_average_daily_volume(file_paths, output_path=None):\n",
    "\n",
    "    \"\"\"\n",
    "    Calculates the daily average trading volume for each ticker from multiple Tardis trades files.\n",
    "\n",
    "    Parameters:\n",
    "    - file_paths (list): List of full paths to the .csv.gz files.\n",
    "    - output_path (str, optional): Path to save the final combined CSV file.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame with daily average volume for each ticker.\n",
    "    \"\"\"\n",
    "\n",
    "    average_daily_volumes = []\n",
    "\n",
    "    print(\"ðŸ“Š Processing trade files for daily volume:\")\n",
    "    for file_path in tqdm(file_paths, desc=\"Files processed\"):\n",
    "\n",
    "        #Create a DataFrame with the ticker name and average daily volume\n",
    "        ticker = file_path.split('/')[-1].split('_')[0]\n",
    "\n",
    "        print(\"PROCESSING TICKER :\", ticker)\n",
    "\n",
    "        # Read file\n",
    "        df = pd.read_csv(file_path, compression='gzip')\n",
    "\n",
    "        # Drop unused columns\n",
    "        df = df.drop(columns=['exchange', 'local_timestamp', 'id'])\n",
    "\n",
    "        # Convert timestamp from microseconds\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='us')\n",
    "\n",
    "        # Calculate USD volume\n",
    "        df['usd_volume'] = df['price'] * df['amount']\n",
    "\n",
    "        # Resample to 1-day intervals and sum volumes\n",
    "        daily_volumes = df.set_index('timestamp')['usd_volume'].resample('1D').sum().reset_index()\n",
    "\n",
    "        #Calculate the average daily volume\n",
    "        average_daily_volume = daily_volumes['usd_volume'].mean()\n",
    "\n",
    "        # print(f\"Average daily volume for {ticker} below:\")\n",
    "        # print(average_daily_volume)\n",
    "\n",
    "        average_daily_volume_df = pd.DataFrame({\n",
    "            'ticker': ticker,\n",
    "            'average_daily_volume': average_daily_volume\n",
    "        }, index=[0])\n",
    "\n",
    "        # Store for merging later\n",
    "        average_daily_volumes.append(average_daily_volume_df)\n",
    "\n",
    "\n",
    "    print(\"ðŸ”— Merging DataFrames...\")\n",
    "    #Combine all average daily volumes for each ticker into a single DataFrame\n",
    "\n",
    "    #Concatenate all DataFrames\n",
    "    df_combined_volumes = pd.concat(average_daily_volumes, ignore_index=True)\n",
    "    \n",
    "    if output_path:\n",
    "        df_combined_volumes.to_csv(output_path, index=False, compression='gzip')\n",
    "        print(f\"âœ… Final combined volume file saved to {output_path}\")\n",
    "\n",
    "    return df_combined_volumes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get all your combined files\n",
    "# file_paths = glob(\"tardis_data/combined/*_2024_Q1.csv.gz\") #Glob matches all files in the directory that end with _2024_Q1.csv.gz\n",
    "\n",
    "# df_combined_volumes = calculate_average_daily_volume(file_paths, output_path='tardis_data/volumes/average_daily_volumes_2024_Q1.csv.gz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# volumes = pd.read_csv('tardis_data/volumes/average_daily_volumes_2024_Q1.csv.gz', compression='gzip')\n",
    "\n",
    "# #Order volumes by average daily volume\n",
    "# volumes = volumes.sort_values(by='average_daily_volume', ascending=False)\n",
    "\n",
    "# #Print entire DataFrame with no truncation\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_columns', None)\n",
    "\n",
    "# volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all your combined files\n",
    "file_paths = glob(\"tardis_data/combined/*_July_To_September.csv.gz\") #Glob matches all files in the directory that end with _2024_Q1.csv.gz\n",
    "\n",
    "# Combine and save\n",
    "df_final = combine_tardis_trades(file_paths, output_path=\"tardis_data/combined_July_To_September/combined_2024_5m.csv.gz\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
