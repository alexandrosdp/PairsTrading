{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('tardis_data/combined/NEXOUSDT_2024_Q1.csv.gz', compression='gzip')\n",
    "\n",
    "# # Drop unused columns\n",
    "# df = df.drop(columns=['exchange', 'local_timestamp', 'id'])\n",
    "\n",
    "# # Convert timestamp from microseconds\n",
    "# df['timestamp'] = pd.to_datetime(df['timestamp'], unit='us')\n",
    "\n",
    "# # Reorder columns\n",
    "# df = df[['timestamp', 'symbol', 'side', 'price', 'amount']]\n",
    "\n",
    "# # Set timestamp as index\n",
    "# df = df.set_index('timestamp').sort_index()\n",
    "\n",
    "# # Resample to 1-minute intervals\n",
    "# df_1min = df.resample('5min').agg({\n",
    "#     'price': 'last',\n",
    "#     'amount': 'sum',\n",
    "#     'side': 'last',\n",
    "#     'symbol': 'last'\n",
    "# }).dropna().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(df_1min['price'].head(100), color='blue', label='Price')\n",
    "# plt.xlabel('Time')\n",
    "# plt.ylabel('Price')\n",
    "# plt.title('Price over Time')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_liquid = pd.read_csv('tardis_data/combined/NEXOUSDT_2024_Q1.csv.gz', compression='gzip')\n",
    "\n",
    "# # Drop unused columns\n",
    "# df_liquid = df_liquid.drop(columns=['exchange', 'local_timestamp', 'id'])\n",
    "\n",
    "# # Convert timestamp from microseconds\n",
    "# df_liquid['timestamp'] = pd.to_datetime(df_liquid['timestamp'], unit='us')\n",
    "\n",
    "# # Reorder columns\n",
    "# df_liquid = df_liquid[['timestamp', 'symbol', 'side', 'price', 'amount']]\n",
    "\n",
    "# # Set timestamp as index\n",
    "# df_liquid = df_liquid.set_index('timestamp').sort_index()\n",
    "\n",
    "# # Resample to 1-minute intervals\n",
    "# df_liquid_1min = df_liquid.resample('5min').agg({\n",
    "#     'price': 'last',\n",
    "#     'amount': 'sum',\n",
    "#     'side': 'last',\n",
    "#     'symbol': 'last'\n",
    "# }).dropna().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(df_liquid_1min['price'].head(100), color='blue', label='Price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Count the number of times the price does not change over a 1-minute, 2-minute, and 5-minute interval\n",
    "\n",
    "# # Count 1-minute price holds\n",
    "# df_1m = df.set_index('timestamp').resample('1min').last().dropna().reset_index()\n",
    "# no_change_1m = (df['price'].diff() == 0).sum()\n",
    "\n",
    "# # Count 3-minute price holds\n",
    "# df_3m = df.set_index('timestamp').resample('3min').last().dropna().reset_index()\n",
    "# no_change_3m = (df_3m['price'].diff() == 0).sum()\n",
    "\n",
    "# # Count 5-minute price holds\n",
    "# df_5m = df.set_index('timestamp').resample('5min').last().dropna().reset_index()\n",
    "# no_change_5m = (df_5m['price'].diff() == 0).sum()\n",
    "\n",
    "# #Count 30-minute price holds\n",
    "# df_30m = df.set_index('timestamp').resample('30min').last().dropna().reset_index()\n",
    "# no_change_30m = (df_30m['price'].diff() == 0).sum()\n",
    "\n",
    "# print(f\"Price unchanged:\")\n",
    "# print(f\"  Over 1-minute intervals: {no_change_1m}\")\n",
    "# print(f\"  Over 3-minute intervals: {no_change_3m}\")\n",
    "# print(f\"  Over 5-minute intervals: {no_change_5m}\")\n",
    "# print(f\"  Over 30-minute intervals: {no_change_30m}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def combine_tardis_trades(file_paths, output_path=None):\n",
    "#     \"\"\"\n",
    "#     Combines multiple Tardis trades files (compressed .csv.gz) into a single DataFrame\n",
    "#     with 1-minute resampled last prices for each symbol.\n",
    "\n",
    "#     Parameters:\n",
    "#     - file_paths (list): List of full paths to the .csv.gz files.\n",
    "#     - output_path (str, optional): Path to save the final combined CSV file.\n",
    "\n",
    "#     Returns:\n",
    "#     - pd.DataFrame: Combined DataFrame with timestamp and resampled price columns per symbol.\n",
    "#     \"\"\"\n",
    "\n",
    "#     resampled_dfs = []\n",
    "\n",
    "#     print(\"ðŸ“Š Processing trade files:\")\n",
    "#     for file_path in tqdm(file_paths, desc=\"Files processed\"):\n",
    "#         # Read file\n",
    "#         df = pd.read_csv(file_path, compression='gzip')\n",
    "\n",
    "#         # Drop unused columns\n",
    "#         df = df.drop(columns=['exchange', 'local_timestamp', 'id'])\n",
    "\n",
    "#         # Convert timestamp from microseconds\n",
    "#         df['timestamp'] = pd.to_datetime(df['timestamp'], unit='us')\n",
    "\n",
    "#         # Reorder columns\n",
    "#         df = df[['timestamp', 'symbol', 'side', 'price', 'amount']]\n",
    "\n",
    "#         # Set timestamp as index\n",
    "#         df = df.set_index('timestamp').sort_index()\n",
    "\n",
    "#         # Resample to 1-minute intervals\n",
    "#         df_1min = df.resample('5min').agg({\n",
    "#             'price': 'last',\n",
    "#             'amount': 'sum',\n",
    "#             'side': 'last',\n",
    "#             'symbol': 'last'\n",
    "#         }).dropna().reset_index()\n",
    "\n",
    "#         # Rename price column to last_price\n",
    "#         df_1min = df_1min.rename(columns={'price': 'last_price'})\n",
    "\n",
    "#         # Rename for final merge: SYMBOL/USDT_2024_1m\n",
    "#         symbol = df_1min['symbol'].iloc[0]\n",
    "#         df_1min = df_1min.rename(columns={'last_price': f'{symbol}_2024_5m'})\n",
    "\n",
    "#         # Reduce to timestamp + price column\n",
    "#         df_1min = df_1min[['timestamp', f'{symbol}_2024_5m']]\n",
    "\n",
    "#         # Store for merging later\n",
    "#         resampled_dfs.append(df_1min)\n",
    "\n",
    "#     print(\"ðŸ”— Merging DataFrames...\")\n",
    "#     df_combined = reduce(lambda left, right: pd.merge(left, right, on='timestamp', how='inner'), resampled_dfs)\n",
    "\n",
    "#     if output_path:\n",
    "#         df_combined.to_csv(output_path, index=False, compression='gzip')\n",
    "#         print(f\"âœ… Final combined file saved to {output_path}\")\n",
    "\n",
    "#     return df_combined\n",
    "\n",
    "def combine_tardis_trades(file_paths, output_path=None):\n",
    "    \"\"\"\n",
    "    Combines large Tardis trades files into a single DataFrame with 5-minute\n",
    "    resampled last prices per symbol. Optimized for low memory usage.\n",
    "    \"\"\"\n",
    "\n",
    "    combined_df = None  # Start with nothing\n",
    "\n",
    "    print(\"ðŸ“Š Processing and merging trade files one-by-one:\")\n",
    "    for file_path in tqdm(file_paths, desc=\"Files processed\"):\n",
    "        # Read and process each file\n",
    "        df = pd.read_csv(file_path, compression='gzip')\n",
    "\n",
    "        df = df.drop(columns=['exchange', 'local_timestamp', 'id'])\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'], unit='us')\n",
    "        df = df[['timestamp', 'symbol', 'side', 'price', 'amount']]\n",
    "        df = df.set_index('timestamp').sort_index()\n",
    "\n",
    "        df_1min = df.resample('5min').agg({\n",
    "            'price': 'last',\n",
    "            'amount': 'sum',\n",
    "            'side': 'last',\n",
    "            'symbol': 'last'\n",
    "        }).dropna().reset_index()\n",
    "\n",
    "        symbol = df_1min['symbol'].iloc[0]\n",
    "        price_col = f'{symbol}_2024_5m'\n",
    "        df_1min = df_1min.rename(columns={'price': price_col})\n",
    "        df_1min = df_1min[['timestamp', price_col]]\n",
    "        \n",
    "        #Merge is now done incrementally\n",
    "        # Merge into combined_df\n",
    "        if combined_df is None:\n",
    "            combined_df = df_1min\n",
    "        else:\n",
    "            combined_df = pd.merge(combined_df, df_1min, on='timestamp', how='inner')\n",
    "\n",
    "    # Check if output path exits and if not, create it\n",
    "    if output_path:\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        combined_df.to_csv(output_path, index=False, compression='gzip')\n",
    "        print(f\"âœ… Final combined file saved to {output_path}\")\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Create a function to calculate the daily average trading volume for each ticker\n",
    "\n",
    "# def calculate_average_daily_volume(file_paths, output_path=None):\n",
    "\n",
    "#     \"\"\"\n",
    "#     Calculates the daily average trading volume for each ticker from multiple Tardis trades files.\n",
    "\n",
    "#     Parameters:\n",
    "#     - file_paths (list): List of full paths to the .csv.gz files.\n",
    "#     - output_path (str, optional): Path to save the final combined CSV file.\n",
    "\n",
    "#     Returns:\n",
    "#     - pd.DataFrame: DataFrame with daily average volume for each ticker.\n",
    "#     \"\"\"\n",
    "\n",
    "#     average_daily_volumes = []\n",
    "\n",
    "#     print(\"ðŸ“Š Processing trade files for daily volume:\")\n",
    "#     for file_path in tqdm(file_paths, desc=\"Files processed\"):\n",
    "\n",
    "#         #Create a DataFrame with the ticker name and average daily volume\n",
    "#         ticker = file_path.split('/')[-1].split('_')[0]\n",
    "\n",
    "#         print(\"PROCESSING TICKER :\", ticker)\n",
    "\n",
    "#         # Read file\n",
    "#         df = pd.read_csv(file_path, compression='gzip')\n",
    "\n",
    "#         # Drop unused columns\n",
    "#         df = df.drop(columns=['exchange', 'local_timestamp', 'id'])\n",
    "\n",
    "#         # Convert timestamp from microseconds\n",
    "#         df['timestamp'] = pd.to_datetime(df['timestamp'], unit='us')\n",
    "\n",
    "#         # Calculate USD volume\n",
    "#         df['usd_volume'] = df['price'] * df['amount']\n",
    "\n",
    "#         # Resample to 1-day intervals and sum volumes\n",
    "#         daily_volumes = df.set_index('timestamp')['usd_volume'].resample('1D').sum().reset_index()\n",
    "\n",
    "#         #Calculate the average daily volume\n",
    "#         average_daily_volume = daily_volumes['usd_volume'].mean()\n",
    "\n",
    "#         # print(f\"Average daily volume for {ticker} below:\")\n",
    "#         # print(average_daily_volume)\n",
    "\n",
    "#         average_daily_volume_df = pd.DataFrame({\n",
    "#             'ticker': ticker,\n",
    "#             'average_daily_volume': average_daily_volume\n",
    "#         }, index=[0])\n",
    "\n",
    "#         # Store for merging later\n",
    "#         average_daily_volumes.append(average_daily_volume_df)\n",
    "\n",
    "\n",
    "#     print(\"ðŸ”— Merging DataFrames...\")\n",
    "#     #Combine all average daily volumes for each ticker into a single DataFrame\n",
    "\n",
    "#     #Concatenate all DataFrames\n",
    "#     df_combined_volumes = pd.concat(average_daily_volumes, ignore_index=True)\n",
    "\n",
    "#     # Check if output_path exists and if not, create it\n",
    "#     output_dir = os.path.dirname(output_path)\n",
    "#     if output_dir and not os.path.exists(output_dir):\n",
    "#         os.makedirs(output_dir)\n",
    "\n",
    "#     df_combined_volumes.to_csv(output_path, index=False, compression='gzip')\n",
    "#     print(f\"âœ… Final combined volume file saved to {output_path}\")\n",
    "\n",
    "\n",
    "#     return df_combined_volumes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get all your combined files\n",
    "# file_paths = glob(\"tardis_data/combined/*_2024_Q1.csv.gz\") #Glob matches all files in the directory that end with _2024_Q1.csv.gz\n",
    "\n",
    "# df_combined_volumes = calculate_average_daily_volume(file_paths, output_path='tardis_data/volumes/average_daily_volumes_2024_Q1.csv.gz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# volumes = pd.read_csv('tardis_data/volumes/average_daily_volumes_2024_Q1.csv.gz', compression='gzip')\n",
    "\n",
    "# #Order volumes by average daily volume\n",
    "# volumes = volumes.sort_values(by='average_daily_volume', ascending=False)\n",
    "\n",
    "# #Print entire DataFrame with no truncation\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_columns', None)\n",
    "\n",
    "# volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Processing and merging trade files one-by-one:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Files processed: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [01:09<00:00, 17.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Final combined file saved to tardis_data/combined_JAN_To_DEC/combined_2024_new_5m.csv.gz\n"
     ]
    }
   ],
   "source": [
    "# Get all your combined files\n",
    "file_paths = glob(\"tardis_data/combined/*_Jan_To_December.csv.gz\") #Glob matches all files in the directory that end with _2024_Q1.csv.gz\n",
    "\n",
    "# Combine and save\n",
    "df_final = combine_tardis_trades(file_paths, output_path=\"tardis_data/combined_JAN_To_DEC/combined_2024_new_5m.csv.gz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_final.to_csv('tardis_data/combined_October_To_March/combined_2024_2025_5m.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.read_csv(\"tardis_data/combined_JAN_To_DEC/combined_2024_new_5m.csv.gz\", compression='gzip')\n",
    "\n",
    "old_df = pd.read_csv(\"tardis_data/final_training_set/final_training_set_5min_2024.csv\")\n",
    "\n",
    "#Merge the two DataFrames on timestamp using an inner join\n",
    "merged_df = pd.merge(new_df, old_df, on='timestamp', how='inner')\n",
    "#Save the merged DataFrame to a new CSV file\n",
    "merged_df.to_csv(\"tardis_data/final_training_set/final_training_set_top_10.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>SANDUSDT_2024_5m</th>\n",
       "      <th>MANAUSDT_2024_5m</th>\n",
       "      <th>AXSUSDT_2024_5m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-01 00:00:00</td>\n",
       "      <td>0.6006</td>\n",
       "      <td>0.5239</td>\n",
       "      <td>8.880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-01-01 00:05:00</td>\n",
       "      <td>0.5971</td>\n",
       "      <td>0.5228</td>\n",
       "      <td>8.870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-01-01 00:10:00</td>\n",
       "      <td>0.5979</td>\n",
       "      <td>0.5228</td>\n",
       "      <td>8.870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-01-01 00:15:00</td>\n",
       "      <td>0.5941</td>\n",
       "      <td>0.5213</td>\n",
       "      <td>8.850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-01-01 00:20:00</td>\n",
       "      <td>0.5947</td>\n",
       "      <td>0.5218</td>\n",
       "      <td>8.850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103612</th>\n",
       "      <td>2024-12-31 23:35:00</td>\n",
       "      <td>0.5445</td>\n",
       "      <td>0.4651</td>\n",
       "      <td>6.195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103613</th>\n",
       "      <td>2024-12-31 23:40:00</td>\n",
       "      <td>0.5451</td>\n",
       "      <td>0.4661</td>\n",
       "      <td>6.199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103614</th>\n",
       "      <td>2024-12-31 23:45:00</td>\n",
       "      <td>0.5466</td>\n",
       "      <td>0.4669</td>\n",
       "      <td>6.214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103615</th>\n",
       "      <td>2024-12-31 23:50:00</td>\n",
       "      <td>0.5461</td>\n",
       "      <td>0.4669</td>\n",
       "      <td>6.219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103616</th>\n",
       "      <td>2024-12-31 23:55:00</td>\n",
       "      <td>0.5461</td>\n",
       "      <td>0.4670</td>\n",
       "      <td>6.222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>103617 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  timestamp  SANDUSDT_2024_5m  MANAUSDT_2024_5m  \\\n",
       "0       2024-01-01 00:00:00            0.6006            0.5239   \n",
       "1       2024-01-01 00:05:00            0.5971            0.5228   \n",
       "2       2024-01-01 00:10:00            0.5979            0.5228   \n",
       "3       2024-01-01 00:15:00            0.5941            0.5213   \n",
       "4       2024-01-01 00:20:00            0.5947            0.5218   \n",
       "...                     ...               ...               ...   \n",
       "103612  2024-12-31 23:35:00            0.5445            0.4651   \n",
       "103613  2024-12-31 23:40:00            0.5451            0.4661   \n",
       "103614  2024-12-31 23:45:00            0.5466            0.4669   \n",
       "103615  2024-12-31 23:50:00            0.5461            0.4669   \n",
       "103616  2024-12-31 23:55:00            0.5461            0.4670   \n",
       "\n",
       "        AXSUSDT_2024_5m  \n",
       "0                 8.880  \n",
       "1                 8.870  \n",
       "2                 8.870  \n",
       "3                 8.850  \n",
       "4                 8.850  \n",
       "...                 ...  \n",
       "103612            6.195  \n",
       "103613            6.199  \n",
       "103614            6.214  \n",
       "103615            6.219  \n",
       "103616            6.222  \n",
       "\n",
       "[103617 rows x 4 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Final Training set\n",
    "# #---\n",
    "# data_jan_to_june = pd.read_csv('tardis_data/final_in_sample_dataset/final_in_sample_dataset_5min_2024.csv', parse_dates=True)\n",
    "# data_jan_to_june = data_jan_to_june[['timestamp','SANDUSDT_2024_5m','MANAUSDT_2024_5m', 'AXSUSDT_2024_5m']]\n",
    "\n",
    "# data_july_to_september = pd.read_csv('tardis_data/final_out_of_sample_dataset_5min/final_out_of_sample_dataset_5min.csv', parse_dates=True)\n",
    "\n",
    "# data_october_to_march = pd.read_csv('tardis_data/combined_October_To_March/combined_2024_2025_5m.csv.gz', parse_dates=True)\n",
    "\n",
    "# #Arrange columns in the same order as the training set\n",
    "# data_october_to_march = data_october_to_march[['timestamp', 'SANDUSDT_2024_5m', 'MANAUSDT_2024_5m', 'AXSUSDT_2024_5m']]\n",
    "\n",
    "# data_october_to_december = data_october_to_march[data_october_to_march['timestamp'] < '2025-01-01']\n",
    "\n",
    "# final_training_set = pd.concat([data_jan_to_june, data_july_to_september, data_october_to_december], ignore_index=True)\n",
    "\n",
    "# final_training_set.to_csv('tardis_data/final_training_set/final_training_set_5min_2024.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_training_set_read = pd.read_csv('tardis_data/final_training_set/final_training_set_5min_2024.csv', parse_dates=True)\n",
    "# final_training_set_read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Final Testing set\n",
    "# #---\n",
    "\n",
    "# data_january_to_march_2025 = data_october_to_march[data_october_to_march['timestamp'] >= '2025-01-01']\n",
    "# final_test_set = data_january_to_march_2025\n",
    "# final_test_set.to_csv('tardis_data/final_testing_set/final_test_set_5min_2025.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_test_set_read = pd.read_csv('tardis_data/final_testing_set/final_test_set_5min_2025.csv', parse_dates=True)\n",
    "# final_test_set_read"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
