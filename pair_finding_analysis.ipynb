{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_pre_processing import *\n",
    "from pair_finder import *\n",
    "from back_tester import *\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = pd.read_csv(\"binance_data/From_Paper/2023/1m/merged_From_Paper_closing_prices_OCT_NOV.csv\", index_col=0, parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the closing prices of the merged data using different y axis\n",
    "\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "#Set size of the plot\n",
    "fig.set_size_inches(12, 8)\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Time')\n",
    "ax1.set_ylabel('Closing Price', color=color)\n",
    "ax1.plot(prices.index, prices['BTCEUR_2023_1m'], color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('Closing Price', color=color)\n",
    "ax2.plot(prices.index, prices['BTCGBP_2023_1m'], color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate correlation matrix\n",
    "correlation_matrix = prices.corr()\n",
    "correlation_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test for pairs\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ## Apply correlation filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Pre-filter: Calculate correlation matrix and select pairs with high correlation\n",
    "corr_threshold = 0.8\n",
    "corr_matrix, high_corr_pairs = filter_high_correlation_pairs(prices, threshold=corr_threshold)\n",
    "print(f\"\\nPairs with correlation >= {corr_threshold}:\")\n",
    "for pair in high_corr_pairs:\n",
    "    print(f\"{pair[0]} & {pair[1]}: correlation = {pair[2]:.4f}\")\n",
    "\n",
    "# # Cointegration test\n",
    "# cointegrated_pairs, pvalue_matrix, residuals_df = find_cointegrated_pairs(prices, high_corr_pairs,significance=0.05)\n",
    "# #cointegrated_pairs, window_results = find_cointegrated_pairs_windows(prices, high_corr_pairs, significance=0.05, window_size=720, min_pass_fraction=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_corr_pairs = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ## Test cointegration in windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Params\n",
    "window_size = 1440\n",
    "min_pass_fraction = 0.5\n",
    "significance = 0.05\n",
    "\n",
    "#Find cointegrated pairs\n",
    "cointegrated_pairs, window_results = find_cointegrated_pairs_windows(prices, high_corr_pairs, significance, window_size, min_pass_fraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Window results\n",
    "window_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot windows\n",
    "\n",
    "# windows = split_price_series_into_windows(prices, window_size=1440)\n",
    "\n",
    "# #For ICP/USDT_2024_1h', 'ADA/USDT_2024_1h\n",
    "# plot_spread_in_windows('BTCEUR_2023_1m', 'BTCGBP_2023_1m', windows, window_results, significance=0.05) #These spreads look like they are standardised, but they are not. They are centered around 0 due to the inclusion of the intercept term in the cointegration test. The spread is still in the original units of the data.\n",
    "# #plot_spread_in_windows('S1', 'S2', windows, window_results, significance=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### Check resiuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #If you suspect daily intraday patterns, using 24 lags is a natural choice: it checks each hour in a 24-hour cycle for dependence.\n",
    "# #If your sample is large enough (covering many days/weeks), 24 lags is typically enough to detect standard intraday correlation\n",
    "# analyze_residuals(residuals_df, lags = 24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation of cointegration methods using data designed for cointegration\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# def generate_cointegrated_data(\n",
    "#     n=720,  # number of data points\n",
    "#     alpha=5.0,\n",
    "#     beta=1.5,\n",
    "#     phi=0.8, #manually set to 0.8 so that the spread is stationary (no unit root)\n",
    "#     seed=42\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Generate two cointegrated series: S1 (random walk) and S2 (linear function of S1 + stationary AR(1) noise).\n",
    "    \n",
    "#     Parameters:\n",
    "#         n (int): Number of observations (e.g. 720 for ~1 month of hourly data).\n",
    "#         alpha (float): Intercept term for S2.\n",
    "#         beta (float): S2 coefficient for S1.\n",
    "#         phi (float): AR(1) coefficient for the noise in S2 - beta*S1. Must be <1 in abs value for stationarity.\n",
    "#         seed (int): Random seed for reproducibility.\n",
    "\n",
    "#     Returns:\n",
    "#         pd.DataFrame with columns ['S1', 'S2'] representing the two cointegrated time series.\n",
    "#     \"\"\"\n",
    "#     rng = np.random.default_rng(seed)\n",
    "    \n",
    "#     # 1) Generate S1 as a random walk\n",
    "#     #   S1_t = S1_{t-1} + e1_t\n",
    "#     e1 = rng.normal(loc=0.0, scale=1.0, size=n)\n",
    "#     S1 = np.cumsum(e1)  # cumsum => random walk\n",
    "    \n",
    "#     # 2) Generate a stationary AR(1) noise for the spread\n",
    "#     #   ARnoise_t = phi * ARnoise_{t-1} + e2_t\n",
    "#     e2 = rng.normal(loc=0.0, scale=1.0, size=n)\n",
    "#     ARnoise = np.zeros(n)\n",
    "#     for t in range(1, n):\n",
    "#         ARnoise[t] = phi * ARnoise[t-1] + e2[t]\n",
    "#     # ARnoise is stationary if |phi| < 1\n",
    "    \n",
    "#     # 3) Define S2 = alpha + beta*S1 + AR(1) noise\n",
    "#     S2 = alpha + beta*S1 + ARnoise\n",
    "    \n",
    "#     # Put them in a DataFrame\n",
    "#     df = pd.DataFrame({'S1': S1, 'S2': S2})\n",
    "#     return df\n",
    "\n",
    "# # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     df_cointegrated = generate_cointegrated_data(n=720)\n",
    "#     print(df_cointegrated.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Suppose df has columns: 'S1', 'S2'\n",
    "# alpha = 5.0\n",
    "# beta = 1.5\n",
    "\n",
    "# # Calculate the spread: S2 - (alpha + beta * S1)\n",
    "# df_cointegrated['spread'] = df_cointegrated['S2'] - alpha - beta * df_cointegrated['S1']\n",
    "\n",
    "# # Plot\n",
    "# plt.figure(figsize=(10,6))\n",
    "# plt.plot(df_cointegrated['spread'], label='Spread = S2 - alpha - beta*S1')\n",
    "# plt.axhline(df_cointegrated['spread'].mean(), color='red', linestyle='--', label='Mean')\n",
    "# plt.title('Spread Over Time')\n",
    "# plt.legend()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cointegrated_pairs, window_results = find_cointegrated_pairs_windows(df_cointegrated, high_corr_pairs, significance=0.05, window_size=240, min_pass_fraction=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# window_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
